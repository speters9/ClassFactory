import logging
from typing import Any, Dict

from langchain_core.messages import SystemMessage
from langchain_core.prompts import (ChatPromptTemplate,
                                    HumanMessagePromptTemplate, PromptTemplate)

from class_factory.utils.tools import logger_setup, retry_on_json_decode_error


class Validator:
    """A class for validating responses generated by an LLM (Language Model).

    The Validator checks the accuracy, completeness, and relevance of the LLM's response
    to ensure it meets the requirements specified in the task prompt. Validation results
    include a score, status, reasoning, and any additional guidance.
    """

    def __init__(self, llm: Any, parser: Any, temperature: float = 0.6, log_level=logging.INFO) -> None:
        """
        Initialize the Validator instance.

        Args:
            llm (Any): The language model instance used to generate responses.
            parser (Any): The parser instance to process the LLM's output. Common parsers might include JsonOutputParser with associated pydantic object.
            temperature (float, optional): The temperature setting for response variability. Defaults to 0.6.
            log_level (int, optional): The logging level to use. Defaults to logging.INFO.
        """
        self.llm = llm
        self.parser = parser
        self.llm.temperature = temperature
        self.logger = logger_setup(logger_name="validator", log_level=log_level)

        # Send the prompt to the LLM

    @retry_on_json_decode_error()
    def validate(self, task_description: str, generated_response: str, specific_guidance: str = "", min_eval_score: int = 8) -> Dict[str, Any]:
        """
        Validates a generated response by providing the task description, the generated response, and any specific guidance for evaluation.

        Args:
            task_description (str): Description of the task that the LLM was originally given.
            generated_response (str): The output generated by the LLM that needs validation.
            specific_guidance (str, optional): Additional guidance for the LLM during validation. Defaults to "".
            min_eval_score (int, optional): Minimum score required for a valid response. Affects the status in the return value. Defaults to 8.

        Returns:
            Dict[str, Any]: Validation result containing:
                - evaluation_score (float): Score from 0.0 to 10.0 based on response quality
                - status (int): 1 if score >= min_eval_score, 0 otherwise
                - reasoning (str): Explanation of the assigned score
                - additional_guidance (str): Improvement suggestions if status is 0, empty string otherwise
        """

        system_message = """
            You are an impartial judge tasked with validating another AI's response based on specific task criteria.
            You will be provided the AI's task and context as well as its response. Your job is to rate the AI's response.
            Focus on the AI responses's completeness, accuracy, and adherence to any required structure. Be as objective as possible.
            Return your results only in JSON format.
            """

        human_message_template = """
            Evaluate the AI-generated response according to the following criteria:

            ### Evaluation Criteria:
            - Completeness: Ensure that the response adequately covers the main elements specified in the task prompt without requiring exhaustive detail.
            - Accuracy: Confirm that no information is invented, and the content reasonably aligns with the task requirements.
            - Consistency: Verify that the format adheres to any required structure (e.g., JSON) and that formatting instructions are followed.

            {specific_guidance}

            ### Task Details:
                **Original Task Description:**
                "{task_description}"

                **Generated Response:**
                "{generated_response}"

            ### Your Response:
                Respond in JSON format with:
                    - `"evaluation_score"`: A score from 0.0 to 10.0 based on how well the response meets the task requirements. Use the following scoring criteria:
                        - 9-10: Fully accurate and complete, with minor or no issues; fulfills task requirements.
                        - 7-8: Mostly accurate, but minor details may be lacking; generally sufficient unless the min evaluation_score of {min_eval_score} is higher.
                        - 5-6: Some significant omissions or inaccuracies that impact response clarity or criteria coverage.
                        - Below 5: Major errors or omissions that fail to meet task requirements.
                    - `"status"`: 1 if the evaluation score is greater than or equal to {min_eval_score}, 0 otherwise.
                    - `"reasoning"`: A brief explanation of why the score was assigned, focusing on task criteria.
                    - `"additional_guidance"`: If "status" is 0, provide guidance to improve the response's alignment with the task requirements. Your guidance will be included in the next prompt to the model. Else return an empty string

            ### Example responses:
                **Example if status is 1 (JSON)**:
                ```
                    {{
                        "evaluation_score": 8.5,
                        "status": 1,
                        "reasoning": "The response summarizes the main points from the text accurately and completely, meeting task requirements adequately.",
                        "additional_guidance": ""
                    }}
                ```

                **Example if status is 0 (JSON)**:
                ```
                    {{
                        "evaluation_score": 6.5,
                        "status": 0,
                        "reasoning": "The response covers key points but lacks coverage of some task criteria and has minor formatting inconsistencies.",
                        "additional_guidance": "Ensure each task criterion is addressed sufficiently and format consistently as specified."
                    }}
                ```

            Respond only with valid JSON format containing "evaluation_score", "status", "reasoning", and "additional_guidance".

            ### IMPORTANT: Your response **must** strictly follow the JSON format above. Include only the json in your response.
               If the JSON is invalid or extra text is included, your response will be rejected.
            """

        # Combine system and human messages
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(content=system_message),
                HumanMessagePromptTemplate.from_template(human_message_template),
            ]
        )

        # Format the prompt with the provided data
        chain = prompt | self.llm | self.parser

        # Send the prompt to the LLM
        response = chain.invoke({
            "task_description": task_description,
            "generated_response": generated_response,
            "specific_guidance": specific_guidance,
            "min_eval_score": min_eval_score
        })

        return response


if __name__ == "__main__":
    import json
    import os
    from pathlib import Path

    # env setup
    from dotenv import load_dotenv
    # llm chain setup
    from langchain_community.llms import Ollama
    from langchain_core.output_parsers import JsonOutputParser
    from langchain_openai import ChatOpenAI
    from pyprojroot.here import here

    from class_factory.concept_web.concept_extraction import (
        extract_relationships, summarize_text)
    from class_factory.concept_web.prompts import (relationship_prompt,
                                                   summary_prompt)
    from class_factory.utils.load_documents import LessonLoader
    from class_factory.utils.response_parsers import (Extracted_Relations,
                                                      ValidatorResponse)
    projectDir = here()
    load_dotenv()

    # Path definitions
    user_home = Path.home()
    readingDir = user_home / os.getenv('readingsDir')
    syllabus_path = user_home / os.getenv('syllabus_path')

    # Example usage
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.2,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        api_key=os.getenv('openai_key'),
        organization=os.getenv('openai_org'),
    )

    # llm = Ollama(
    #     model="llama3.1",
    #     temperature=0.2
    # )

    parser = JsonOutputParser(pydantic_object=Extracted_Relations)
    val_parser = JsonOutputParser(pydantic_object=ValidatorResponse)
    course_name = "American Government"

    validator = Validator(llm=llm, parser=val_parser)
    loader = LessonLoader(syllabus_path=syllabus_path,
                          reading_dir=readingDir,
                          slide_dir=None)

    # Load documents and lesson objectives
    for lesson_num in range(19, 20):
        lesson_objectives = loader.extract_lesson_objectives(current_lesson=lesson_num)
        documents = loader.load_lessons(lesson_number_or_range=range(lesson_num, lesson_num + 1))

        for document in documents:
            retries = 0
            additional_guidance = ""
            valid = False
            summary = summarize_text(document, prompt=summary_prompt, course_name=course_name, llm=llm)

            combined_template = PromptTemplate.from_template(relationship_prompt)
            chain = combined_template | llm | parser

            while not valid and retries < 3:
                response = chain.invoke({'course_name': course_name,
                                         'objectives': lesson_objectives,
                                         'text': document,
                                         'additional_guidance': additional_guidance})

                # Clean and parse the JSON output
                if isinstance(response, str):
                    response_cleaned = response.replace("```json", "").replace("```", "")
                    data = json.loads(response_cleaned)  # This may raise JSONDecodeError
                else:
                    data = response

                # Verify that data is a dict
                if not isinstance(data, dict):
                    raise ValueError("Parsed data is not a dictionary.")

                response_str = json.dumps(response).replace("{", "{{").replace("}", "}}")

                validation_prompt = combined_template.format(course_name=course_name,
                                                             objectives=lesson_objectives,
                                                             text=document,
                                                             additional_guidance=additional_guidance).replace("{", "{{").replace("}", "}}")

                val_response = validator.validate(task_description=validation_prompt,
                                                  generated_response=response_str)

                print(f"validation output: {val_response}")
                if int(val_response['status']) == 1:
                    valid = True
                else:
                    retries += 1
                    additional_guidance = val_response.get("additional_guidance", "")
                    validator.logger.warning("Validation failed, attempting retry")

            if valid:
                validator.logger.info("Validation succeeded.")
            else:
                raise ValueError("Validation failed after max retries. Ensure correct prompt and input data. Consider use of a different LLM.")
