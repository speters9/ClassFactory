"""Extract and process concept relationships from educational content.

This module processes lesson readings and objectives to create structured concept relationships
for visualization and analysis.

The process follows these steps:

1. Text Summarization
    * Condenses lesson readings using language models
    * Guided by course-specific prompts

2. Relationship Extraction
    * Identifies key concepts from summarized text
    * Maps relationships between concepts
    * Uses lesson objectives to guide extraction

3. Concept Normalization
    * Standardizes concept names
    * Consolidates similar concepts using embeddings
    * Resolves duplicate concepts

4. Output Processing
    * Ensures consistent concept naming
    * Prepares relationships for visualization
    * Structures data for downstream use

Dependencies
-----------
* Language Models
    - OpenAI GPT or similar for text processing
    - DistilBERT for concept embeddings
* Core Libraries
    - langchain : For LLM interaction
    - torch : For embedding processing
    - inflect : For concept name normalization
    - transformers : For BERT model access

Example
-------
>>> from class_factory.concept_web.concept_extraction import extract_relationships
>>> text = "Democracy relies on voting rights..."
>>> objectives = "Understand principles of democracy"
>>> relationships = extract_relationships(text, objectives, "Political Science", llm)
>>> processed = process_relationships(relationships)

Notes
-----
This module is designed as part of a larger educational content processing pipeline,
where extracted relationships can be used for concept mapping and visualization.
"""
# %%
# base libraries
import json
# logger setup
import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import inflect
# entity resolution
import torch
import torch.nn.functional as F
# env setup
from dotenv import load_dotenv
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from transformers import DistilBertModel, DistilBertTokenizer

from class_factory.concept_web.prompts import (relationship_prompt,
                                               summary_prompt)
from class_factory.utils.llm_validator import Validator
from class_factory.utils.response_parsers import (ExtractedRelations,
                                                  ValidatorResponse)
from class_factory.utils.tools import logger_setup, retry_on_json_decode_error

# logging.basicConfig(
#     level=logging.INFO,  # Set your desired level
#     format='%(name)s - %(levelname)s - %(message)s'
# )

# %%


def summarize_text(text: str, prompt: ChatPromptTemplate, course_name: str, llm: Any,
                   parser: StrOutputParser = StrOutputParser(), verbose: bool = False) -> str:
    """Summarize the provided text using the specified prompt and objectives.

    Args:
        text (str): The text to be summarized
        prompt (ChatPromptTemplate): The prompt template to guide the summarization
        course_name (str): Name of the course being processed
        llm (Any): Language model instance to use for generating the summary
        parser (StrOutputParser, optional): Parser to handle the output. Defaults to StrOutputParser()
        verbose (bool, optional): Enable detailed logging. Defaults to False

    Returns:
        str: The summary generated by the language model

    Raises:
        Exception: If LLM fails to generate summary
    """
    log_level = logging.INFO if verbose else logging.ERROR
    logger = logger_setup(log_level=log_level)

    chain = prompt | llm | parser
    retries, max_retries = 0, 3
    valid = False

    # Create validator with appropriate parser
    val_parser = JsonOutputParser(pydantic_object=ValidatorResponse)
    validator = Validator(llm=llm, parser=val_parser, log_level=log_level)

    additional_guidance = ""

    while not valid and retries < max_retries:
        summary = chain.invoke({'course_name': course_name,
                                'text': text,
                                'additional_guidance': additional_guidance})
        logger.info(f"Example summary:\n{summary}")

        # Validate the generated summary
        validation_prompt = prompt.format(course_name=course_name,
                                          text=text,
                                          additional_guidance=additional_guidance)

        val_response = validator.validate(task_description=validation_prompt,
                                          generated_response=summary,
                                          min_eval_score=8)

        logger.info(f"validation output: {val_response}")
        if int(val_response['status']) == 1:
            valid = True
        else:
            retries += 1
            additional_guidance = val_response.get("additional_guidance", "")
            logger.warning(f"Summary validation failed on attempt {retries}. Reason: {val_response['reasoning']}")

    if valid:
        logger.debug("Validation succeeded.")
    else:
        raise ValueError("Validation failed after max retries. Ensure correct prompt and input data. Consider use of a different LLM.")

    return summary


@retry_on_json_decode_error()
def extract_relationships(text: str, objectives: str, course_name: str,
                          llm: Any, verbose: bool = False,
                          logger: Optional[logging.Logger] = None) -> List[Tuple[str, str, str]]:
    """Extract key concepts and their relationships from the provided text.

    Args:
        text (str): The summarized text
        objectives (str): Lesson objectives to guide the relationship extraction
        course_name (str): Name of the course (e.g., "American Government")
        llm (Any): Language model to use for generating responses
        verbose (bool, optional): Enable detailed logging. Defaults to False
        logger (Optional[logging.Logger], optional): Logger to use. Defaults to None

    Returns:
        List[Tuple[str, str, str]]: List of tuples containing (concept1, relationship, concept2)

    Raises:
        ValueError: If validation fails after max retries
        JSONDecodeError: If response parsing fails
    """
    log_level = logging.INFO if verbose else logging.WARNING
    logger = logger or logging.getLogger(__name__)
    logger.setLevel(log_level)

    parser = JsonOutputParser(pydantic_object=ExtractedRelations)
    val_parser = JsonOutputParser(pydantic_object=ValidatorResponse)

    if not objectives:
        objectives = "Not provided."

    additional_guidance = ""
    # combined_template = PromptTemplate.from_template(selected_prompt)
    chain = relationship_prompt | llm | parser

    logger.debug(f"""Querying with:\n{relationship_prompt.format(course_name=course_name,
                                                 objectives=objectives,
                                                 text="placeholder",
                                                 additional_guidance="")}""")

    validator = Validator(llm=llm, parser=val_parser, log_level=log_level)
    retries, max_retries = 0, 3
    valid = False

    while not valid and retries < max_retries:
        response = chain.invoke({'course_name': course_name,
                                 'objectives': objectives,
                                 'text': text,
                                 'additional_guidance': additional_guidance})

        # Clean and parse the JSON output
        if isinstance(response, str):
            response_cleaned = response.replace("```json", "").replace("```", "")
            data = json.loads(response_cleaned)  # This may raise JSONDecodeError
        else:
            data = response

        # Verify that data is a dict
        if not isinstance(data, dict):
            logger.error("Parsed data is not a dictionary.")
            raise ValueError("Parsed data is not a dictionary.")

        # Validate responses
        # escape curly braces for langchain invoke with double curlies
        response_str = json.dumps(response).replace("{", "{{").replace("}", "}}")
        validation_prompt = relationship_prompt.format(course_name=course_name,
                                                       objectives=objectives,
                                                       text=text,
                                                       additional_guidance=additional_guidance
                                                       ).replace("{", "{{").replace("}", "}}")

        val_response = validator.validate(task_description=validation_prompt,
                                          generated_response=response_str,
                                          min_eval_score=8)

        logger.info(f"validation output: {val_response}")
        if int(val_response['status']) == 1:
            valid = True
        else:
            retries += 1
            additional_guidance = val_response.get("additional_guidance", "")
            logger.warning(f"Relationship validation failed on attempt {retries}. Reason: {val_response['reasoning']}")

    if valid:
        logger.info("Validation succeeded.")
    else:
        raise ValueError("Validation failed after max retries. Ensure correct prompt and input data. Consider use of a different LLM.")

    # Extract concepts and relationships
    relationships = [tuple(relationship) for relationship in data["relationships"]]
    return relationships


def extract_concepts_from_relationships(relationships: List[Tuple[str, str, str]]) -> List[str]:
    """
    Extract unique concepts from the list of relationships.

    Args:
        relationships (list): List of tuples representing relationships between concepts.

    Returns:
        list: A list of unique concepts.
    """
    concepts = set()  # Use a set to avoid duplicates
    for concept1, _, concept2 in relationships:
        concepts.add(concept1.lower().strip())
        concepts.add(concept2.lower().strip())
    return list(concepts)


def get_embeddings(concepts: List[str]) -> Dict[str, torch.Tensor]:
    """Get normalized embeddings for a list of concepts using DistilBERT.

    Args:
        concepts (List[str]): List of concepts to embed

    Returns:
        Dict[str, torch.Tensor]: Dictionary mapping concepts to their normalized embeddings

    Note:
        Uses DistilBERT's [CLS] token embedding
    """
    tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
    model = DistilBertModel.from_pretrained("distilbert-base-uncased")

    # Tokenize and process concepts in batches
    inputs = tokenizer(concepts, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)

    # Use the [CLS] token's embedding
    embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: (num_concepts, hidden_size)

    # Normalize embeddings
    normalized_embeddings = F.normalize(embeddings, p=2, dim=1)

    # Map concepts directly to embeddings
    return {concept: normalized_embeddings[idx] for idx, concept in enumerate(concepts)}


def normalize_concept(concept: str) -> str:
    """Normalize a single concept."""
    p = inflect.engine()
    words = concept.lower().strip().replace('_', ' ').split()
    normalized_words = [p.singular_noun(word) or word for word in words]
    return " ".join(normalized_words)


def normalize_for_embedding(concepts: Union[str, List[str]]) -> Union[str, List[str]]:
    """Normalize one or more concepts for embedding."""
    if isinstance(concepts, str):
        return normalize_concept(concepts)
    return [normalize_concept(concept) for concept in concepts]


def normalize_for_output(concept: str) -> str:
    """Format concept for output by replacing spaces with underscores."""
    concept_words = [word for word in concept.split() if word != 'is']
    return "_".join(concept_words)


def replace_similar_concepts(existing_concepts: Set[str], new_concept: str,
                             concept_embeddings: Dict[str, torch.Tensor],
                             threshold: float = 0.995) -> str:
    """Replace a new concept with an existing similar concept if found.

    Args:
        existing_concepts (Set[str]): Set of existing concepts
        new_concept (str): The new concept to check
        concept_embeddings (Dict[str, torch.Tensor]): Words and their associated embeddings
        threshold (float, optional): Similarity threshold for replacement. Defaults to 0.995

    Returns:
        str: The existing concept if a match is found, otherwise the new concept

    Note:
        Threshold is high due to the highly related domain-specific nature of concepts
    """
    # Get the embedding of the new concept
    new_embedding = concept_embeddings[new_concept]

    for existing_concept in existing_concepts:
        existing_embedding = concept_embeddings[existing_concept]

        # Compute cosine similarity
        similarity = torch.matmul(new_embedding, existing_embedding).item()

        # If similar, return the existing concept
        if similarity >= threshold:
            return existing_concept

    # If no similar concept is found, return the new concept
    return new_concept


def process_relationships(relationships: List[Tuple[str, str, str]],
                          threshold: float = 0.995,
                          max_retries: int = 3) -> List[Tuple[str, str, str]]:
    """Process and normalize relationships by consolidating similar concepts.

    Args:
        relationships (List[Tuple[str, str, str]]): List of (concept1, relationship, concept2) tuples
        threshold (float, optional): Similarity threshold for replacement. Defaults to 0.995
        max_retries (int, optional): Number of times to try resolving duplicates. Defaults to 3

    Returns:
        List[Tuple[str, str, str]]: Processed relationships with normalized concepts

    Note:
        Increases threshold by 0.0015 on each retry
    """
    # Initialize a set to keep track of all unique concepts
    unique_concepts = set()
    processed_relationships = []

    if not isinstance(relationships[0], tuple):
        relationships = [tuple(relation) for relation in relationships]

    # get concepts and embeddings
    extracted_concepts = extract_concepts_from_relationships(relationships)
    conceptlist = normalize_for_embedding(extracted_concepts)
    concept_embeddings = get_embeddings(conceptlist)

    for c1, relationship, c2 in relationships:
        c1 = normalize_for_embedding(c1)
        c2 = normalize_for_embedding(c2)

        # Replace similar concepts with existing ones
        retries = 0
        concept1 = replace_similar_concepts(unique_concepts, c1, concept_embeddings, threshold)
        concept2 = replace_similar_concepts(unique_concepts, c2, concept_embeddings, threshold)

        # Retry resolution if concepts are identical. Max threshold increase = 0.0045 (up from default 0.0095)
        while concept1 == concept2 and retries < max_retries:
            retries += 1
            concept1 = replace_similar_concepts(unique_concepts, c1, concept_embeddings, threshold + retries * 0.0015)
            concept2 = replace_similar_concepts(unique_concepts, c2, concept_embeddings, threshold + retries * 0.0015)

        # # If still identical, skip or revert to original
        # if concept1 == concept2:
        #     continue  # Skip self-referential relationships

        # Add concepts to the unique set
        unique_concepts.add(concept1)
        unique_concepts.add(concept2)

        # Normalize concepts
        clean_concept1 = normalize_for_output(concept1)
        clean_concept2 = normalize_for_output(concept2)
        clean_relation = normalize_for_output(relationship)

        # Add the relationship to the processed list
        processed_relationships.append((clean_concept1, clean_relation, clean_concept2))

    return processed_relationships


if __name__ == "__main__":
    # llm chain setup
    from langchain_community.llms import Ollama
    from langchain_openai import ChatOpenAI
    from pyprojroot.here import here

    # self-defined utils
    from class_factory.utils.load_documents import LessonLoader
    user_home = Path.home()
    load_dotenv()

    OPENAI_KEY = os.getenv('openai_key')
    OPENAI_ORG = os.getenv('openai_org')

    # Path definitions
    readingDir = user_home / os.getenv('readingsDir')
    slideDir = user_home / os.getenv('slideDir')
    syllabus_path = user_home / os.getenv('syllabus_path')
    pdf_syllabus_path = user_home / os.getenv('pdf_syllabus_path')

    projectDir = here()

    parser = JsonOutputParser(pydantic_object=ExtractedRelations)

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        api_key=OPENAI_KEY,
        organization=OPENAI_ORG,
    )

    # llm = Ollama(
    #     model="llama3.1",
    #     temperature=0.5,

    #     )

    relationship_list = []
    conceptlist = []

    loader = LessonLoader(syllabus_path=syllabus_path,
                          reading_dir=readingDir,
                          slide_dir=None)

    # Load documents and lesson objectives
    for lesson_num in range(3, 4):
        print(f"Lesson {lesson_num}")
        lesson_objectives = loader.extract_lesson_objectives(current_lesson=lesson_num, only_current=True)
        documents = loader.load_lessons(lesson_number_or_range=range(lesson_num, lesson_num + 1))

        if not documents:
            continue

        for lsn, readings in documents.items():
            for reading in readings:
                summary = summarize_text(reading,
                                         prompt=summary_prompt,
                                         course_name="American government",
                                         llm=llm,
                                         verbose=True)
                # print(summary)
                relationships = extract_relationships(summary,
                                                      lesson_objectives,
                                                      course_name="American government",
                                                      llm=llm,
                                                      verbose=True)
                print(relationships)
                relationship_list.extend(relationships)

                concepts = extract_concepts_from_relationships(relationships)
                conceptlist.extend(concepts)

        processed_relationships = process_relationships(relationship_list)
